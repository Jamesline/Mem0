---
title: 🤖 Large language models (LLMs)
---

## Overview

Embedchain comes with built-in support for various popular large language models. We handle the complexity of integrating these models for you, allowing you to easily customize your language model interactions through a user-friendly interface.

<CardGroup cols={4}>
  <Card title="OpenAI" href="#openai"></Card>
  <Card title="Google AI" href="#google-ai"></Card>
  <Card title="Azure OpenAI" href="#azure-openai"></Card>
  <Card title="Anthropic" href="#anthropic"></Card>
  <Card title="Cohere" href="#cohere"></Card>
  <Card title="Together" href="#together"></Card>
  <Card title="Ollama" href="#ollama"></Card>
  <Card title="vLLM" href="#vllm"></Card>
  <Card title="GPT4All" href="#gpt4all"></Card>
  <Card title="JinaChat" href="#jinachat"></Card>
  <Card title="Hugging Face" href="#hugging-face"></Card>
  <Card title="Llama2" href="#llama2"></Card>
  <Card title="Vertex AI" href="#vertex-ai"></Card>
  <Card title="Mistral AI" href="#mistral-ai"></Card>
  <Card title="AWS Bedrock" href="#aws-bedrock"></Card>
  <Card title="Groq" href="#groq"></Card>
  <Card title="NVIDIA AI" href="#nvidia-ai"></Card>
  <Card title="Zhipu AI" href="#zhipu-ai"></Card>
</CardGroup>

## OpenAI

To use OpenAI LLM models, you have to set the `OPENAI_API_KEY` environment variable. You can obtain the OpenAI API key from the [OpenAI Platform](https://platform.openai.com/account/api-keys).

Once you have obtained the key, you can use it like this:

```python
import os
from embedchain import App

os.environ['OPENAI_API_KEY'] = 'xxx'

app = App()
app.add("https://en.wikipedia.org/wiki/OpenAI")
app.query("What is OpenAI?")
```

If you are looking to configure the different parameters of the LLM, you can do so by loading the app using a [yaml config](https://github.com/embedchain/embedchain/blob/main/configs/chroma.yaml) file.

<CodeGroup>

```python main.py
import os
from embedchain import App

os.environ['OPENAI_API_KEY'] = 'xxx'

# load llm configuration from config.yaml file
app = App.from_config(config_path="config.yaml")
```

```yaml config.yaml
llm:
  provider: openai
  config:
    model: 'gpt-3.5-turbo'
    temperature: 0.5
    max_tokens: 1000
    top_p: 1
    stream: false
```
</CodeGroup>

### Function Calling
Embedchain supports OpenAI [Function calling](https://platform.openai.com/docs/guides/function-calling) with a single function. It accepts inputs in accordance with the [Langchain interface](https://python.langchain.com/docs/modules/model_io/chat/function_calling#legacy-args-functions-and-function_call).

<Accordion title="Pydantic Model">
  ```python
  from pydantic import BaseModel

  class multiply(BaseModel):
      """Multiply two integers together."""

      a: int = Field(..., description="First integer")
      b: int = Field(..., description="Second integer")
  ```
</Accordion>

<Accordion title="Python function">
  ```python
  def multiply(a: int, b: int) -> int:
      """Multiply two integers together.

      Args:
          a: First integer
          b: Second integer
      """
      return a * b
  ```
</Accordion>
<Accordion title="OpenAI tool dictionary">
  ```python
  multiply = {
    "type": "function",
    "function": {
      "name": "multiply",
      "description": "Multiply two integers together.",
      "parameters": {
        "type": "object",
        "properties": {
          "a": {
            "description": "First integer",
            "type": "integer"
          },
          "b": {
            "description": "Second integer",
            "type": "integer"
          }
        },
        "required": [
          "a",
          "b"
        ]
      }
    }
  }
  ```
</Accordion>

With any of the previous inputs, the OpenAI LLM can be queried to provide the appropriate arguments for the function.

```python
import os
from embedchain import App
from embedchain.llm.openai import OpenAILlm

os.environ["OPENAI_API_KEY"] = "sk-xxx"

llm = OpenAILlm(tools=multiply)
app = App(llm=llm)

result = app.query("What is the result of 125 multiplied by fifteen?")
```

## Google AI

To use Google AI model, you have to set the `GOOGLE_API_KEY` environment variable. You can obtain the Google API key from the [Google Maker Suite](https://makersuite.google.com/app/apikey)

<CodeGroup>
```python main.py
import os
from embedchain import App

os.environ["GOOGLE_API_KEY"] = "xxx"

app = App.from_config(config_path="config.yaml")

app.add("https://www.forbes.com/profile/elon-musk")

response = app.query("What is the net worth of Elon Musk?")
if app.llm.config.stream: # if stream is enabled, response is a generator
    for chunk in response:
        print(chunk)
else:
    print(response)
```

```yaml config.yaml
llm:
  provider: google
  config:
    model: gemini-pro
    max_tokens: 1000
    temperature: 0.5
    top_p: 1
    stream: false

embedder:
  provider: google
  config:
    model: 'models/embedding-001'
    task_type: "retrieval_document"
    title: "Embeddings for Embedchain"
```
</CodeGroup>

## Azure OpenAI

To use Azure OpenAI model, you have to set some of the azure openai related environment variables as given in the code block below:

<CodeGroup>

```python main.py
import os
from embedchain import App

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_BASE"] = "https://xxx.openai.azure.com/"
os.environ["OPENAI_API_KEY"] = "xxx"
os.environ["OPENAI_API_VERSION"] = "xxx"

app = App.from_config(config_path="config.yaml")
```

```yaml config.yaml
llm:
  provider: azure_openai
  config:
    model: gpt-3.5-turbo
    deployment_name: your_llm_deployment_name
    temperature: 0.5
    max_tokens: 1000
    top_p: 1
    stream: false

embedder:
  provider: azure_openai
  config:
    model: text-embedding-ada-002
    deployment_name: you_embedding_model_deployment_name
```
</CodeGroup>

You can find the list of models and deployment name on the [Azure OpenAI Platform](https://oai.azure.com/portal).

## Anthropic

To use anthropic's model, please set the `ANTHROPIC_API_KEY` which you find on their [Account Settings Page](https://console.anthropic.com/account/keys).

<CodeGroup>

```python main.py
import os
from embedchain import App

os.environ["ANTHROPIC_API_KEY"] = "xxx"

# load llm configuration from config.yaml file
app = App.from_config(config_path="config.yaml")
```

```yaml config.yaml
llm:
  provider: anthropic
  config:
    model: 'claude-instant-1'
    temperature: 0.5
    max_tokens: 1000
    top_p: 1
    stream: false
```

</CodeGroup>

## Cohere

Install related dependencies using the following command:

```bash
pip install --upgrade 'embedchain[cohere]'
```

Set the `COHERE_API_KEY` as environment variable which you can find on their [Account settings page](https://dashboard.cohere.com/api-keys).

Once you have the API key, you are all set to use it with Embedchain.

<CodeGroup>

```python main.py
import os
from embedchain import App

os.environ["COHERE_API_KEY"] = "xxx"

# load llm configuration from config.yaml file
app = App.from_config(config_path="config.yaml")
```

```yaml config.yaml
llm:
  provider: cohere
  config:
    model: large
    temperature: 0.5
    max_tokens: 1000
    top_p: 1
```

</CodeGroup>

## Together

Install related dependencies using the following command:

```bash
pip install --upgrade 'embedchain[together]'
```

Set the `TOGETHER_API_KEY` as environment variable which you can find on their [Account settings page](https://api.together.xyz/settings/api-keys).

Once you have the API key, you are all set to use it with Embedchain.

<CodeGroup>

```python main.py
import os
from embedchain import App

os.environ["TOGETHER_API_KEY"] = "xxx"

# load llm configuration from config.yaml file
app = App.from_config(config_path="config.yaml")
```

```yaml config.yaml
llm:
  provider: together
  config:
    model: togethercomputer/RedPajama-INCITE-7B-Base
    temperature: 0.5
    max_tokens: 1000
    top_p: 1
```

</CodeGroup>

## Ollama

Setup Ollama using https://github.com/jmorganca/ollama

<CodeGroup>

```python main.py
import os
from embedchain import App

# load llm configuration from config.yaml file
app = App.from_config(config_path="config.yaml")
```

```yaml config.yaml
llm:
  provider: ollama
  config:
    model: 'llama2'
    temperature: 0.5
    top_p: 1
    stream: true
    base_url: 'http://localhost:11434'
```

</CodeGroup>


## vLLM

Setup vLLM by following instructions given in [their docs](https://docs.vllm.ai/en/latest/getting_started/installation.html).

<CodeGroup>

```python main.py
import os
from embedchain import App

# load llm configuration from config.yaml file
app = App.from_config(config_path="config.yaml")
```

```yaml config.yaml
llm:
  provider: vllm
  config:
    model: 'meta-llama/Llama-2-70b-hf'
    temperature: 0.5
    top_p: 1
    top_k: 10
    stream: true
    trust_remote_code: true
```

</CodeGroup>

## GPT4ALL

Install related dependencies using the following command:

```bash
pip install --upgrade 'embedchain[opensource]'
```

GPT4all is a free-to-use, locally running, privacy-aware chatbot. No GPU or internet required. You can use this with Embedchain using the following code:

<CodeGroup>

```python main.py
from embedchain import App

# load llm configuration from config.yaml file
app = App.from_config(config_path="config.yaml")
```

```yaml config.yaml
llm:
  provider: gpt4all
  config:
    model: 'orca-mini-3b-gguf2-q4_0.gguf'
    temperature: 0.5
    max_tokens: 1000
    top_p: 1
    stream: false

embedder:
  provider: gpt4all
```
</CodeGroup>


## JinaChat

First, set `JINACHAT_API_KEY` in environment variable which you can obtain from [their platform](https://chat.jina.ai/api).

Once you have the key, load the app using the config yaml file:

<CodeGroup>

```python main.py
import os
from embedchain import App

os.environ["JINACHAT_API_KEY"] = "xxx"
# load llm configuration from config.yaml file
app = App.from_config(config_path="config.yaml")
```

```yaml config.yaml
llm:
  provider: jina
  config:
    temperature: 0.5
    max_tokens: 1000
    top_p: 1
    stream: false
```
</CodeGroup>


## Hugging Face


Install related dependencies using the following command:

```bash
pip install --upgrade 'embedchain[huggingface-hub]'
```

First, set `HUGGINGFACE_ACCESS_TOKEN` in environment variable which you can obtain from [their platform](https://huggingface.co/settings/tokens).

You can load the LLMs from Hugging Face using three ways:

- [Hugging Face Hub](#hugging-face-hub)
- [Hugging Face Local Pipelines](#hugging-face-local-pipelines)
- [Hugging Face Inference Endpoint](#hugging-face-inference-endpoint)

### Hugging Face Hub

To load the model from Hugging Face Hub, use the following code:

<CodeGroup>

```python main.py
import os
from embedchain import App

os.environ["HUGGINGFACE_ACCESS_TOKEN"] = "xxx"

config = {
  "app": {"config": {"id": "my-app"}},
  "llm": {
      "provider": "huggingface",
      "config": {
          "model": "bigscience/bloom-1b7",
          "top_p": 0.5,
          "max_length": 200,
          "temperature": 0.1,
      },
  },
}

app = App.from_config(config=config)
```
</CodeGroup>

### Hugging Face Local Pipelines

If you want to load the locally downloaded model from Hugging Face, you can do so by following the code provided below:

<CodeGroup>
```python main.py
from embedchain import App

config = {
  "app": {"config": {"id": "my-app"}},
  "llm": {
      "provider": "huggingface",
      "config": {
          "model": "Trendyol/Trendyol-LLM-7b-chat-v0.1",
          "local": True,  # Necessary if you want to run model locally
          "top_p": 0.5,
          "max_tokens": 1000,
          "temperature": 0.1,
      },
  }
}
app = App.from_config(config=config)
```
</CodeGroup>

### Hugging Face Inference Endpoint

You can also use [Hugging Face Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index#-inference-endpoints) to access custom endpoints. First, set the `HUGGINGFACE_ACCESS_TOKEN` as above.

Then, load the app using the config yaml file:

<CodeGroup>

```python main.py
from embedchain import App

config = {
  "app": {"config": {"id": "my-app"}},
  "llm": {
      "provider": "huggingface",
      "config": {
        "endpoint": "https://api-inference.huggingface.co/models/gpt2",
        "model_params": {"temprature": 0.1, "max_new_tokens": 100}
      },
  },
}
app = App.from_config(config=config)

```
</CodeGroup>

Currently only supports `text-generation` and `text2text-generation` for now [[ref](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html?highlight=huggingfaceendpoint#)].

See langchain's [hugging face endpoint](https://python.langchain.com/docs/integrations/chat/huggingface#huggingfaceendpoint) for more information. 

## Llama2

Llama2 is integrated through [Replicate](https://replicate.com/).  Set `REPLICATE_API_TOKEN` in environment variable which you can obtain from [their platform](https://replicate.com/account/api-tokens).

Once you have the token, load the app using the config yaml file:

<CodeGroup>

```python main.py
import os
from embedchain import App

os.environ["REPLICATE_API_TOKEN"] = "xxx"

# load llm configuration from config.yaml file
app = App.from_config(config_path="config.yaml")
```

```yaml config.yaml
llm:
  provider: llama2
  config:
    model: 'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5'
    temperature: 0.5
    max_tokens: 1000
    top_p: 0.5
    stream: false
```
</CodeGroup>

## Vertex AI

Setup Google Cloud Platform application credentials by following the instruction on [GCP](https://cloud.google.com/docs/authentication/external/set-up-adc). Once setup is done, use the following code to create an app using VertexAI as provider:

<CodeGroup>

```python main.py
from embedchain import App

# load llm configuration from config.yaml file
app = App.from_config(config_path="config.yaml")
```

```yaml config.yaml
llm:
  provider: vertexai
  config:
    model: 'chat-bison'
    temperature: 0.5
    top_p: 0.5
```
</CodeGroup>


## Mistral AI

Obtain the Mistral AI api key from their [console](https://console.mistral.ai/).

<CodeGroup>
 
 ```python main.py
os.environ["MISTRAL_API_KEY"] = "xxx"

app = App.from_config(config_path="config.yaml")

app.add("https://www.forbes.com/profile/elon-musk")

response = app.query("what is the net worth of Elon Musk?")
# As of January 16, 2024, Elon Musk's net worth is $225.4 billion.

response = app.chat("which companies does elon own?")
# Elon Musk owns Tesla, SpaceX, Boring Company, Twitter, and X.

response = app.chat("what question did I ask you already?")
# You have asked me several times already which companies Elon Musk owns, specifically Tesla, SpaceX, Boring Company, Twitter, and X.
```
  
```yaml config.yaml
llm:
  provider: mistralai
  config:
    model: mistral-tiny
    temperature: 0.5
    max_tokens: 1000
    top_p: 1
embedder:
  provider: mistralai
  config:
    model: mistral-embed
```
</CodeGroup>


## AWS Bedrock

### Setup
- Before using the AWS Bedrock LLM, make sure you have the appropriate model access from [Bedrock Console](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/modelaccess).
- You will also need to authenticate the `boto3` client by using a method in the [AWS documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#configuring-credentials)
- You can optionally export an `AWS_REGION`


### Usage

<CodeGroup>

```python main.py
import os
from embedchain import App

os.environ["AWS_ACCESS_KEY_ID"] = "xxx"
os.environ["AWS_SECRET_ACCESS_KEY"] = "xxx"
os.environ["AWS_REGION"] = "us-west-2"

app = App.from_config(config_path="config.yaml")
```

```yaml config.yaml
llm:
  provider: aws_bedrock
  config:
    model: amazon.titan-text-express-v1
    # check notes below for model_kwargs
    model_kwargs:
      temperature: 0.5
      topP: 1
      maxTokenCount: 1000
```
</CodeGroup>

<br />
<Note>
  The model arguments are different for each providers. Please refer to the [AWS Bedrock Documentation](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/providers) to find the appropriate arguments for your model.
</Note>

<br/ >

## Groq

[Groq](https://groq.com/) is the creator of the world's first Language Processing Unit (LPU), providing exceptional speed performance for AI workloads running on their LPU Inference Engine.


### Usage

In order to use LLMs from Groq, go to their [platform](https://console.groq.com/keys) and get the API key.

Set the API key as `GROQ_API_KEY` environment variable or pass in your app configuration to use the model as given below in the example.

<CodeGroup>

```python main.py
import os
from embedchain import App

# Set your API key here or pass as the environment variable
groq_api_key = "gsk_xxxx"

config = {
    "llm": {
        "provider": "groq",
        "config": {
            "model": "mixtral-8x7b-32768",
            "api_key": groq_api_key,
            "stream": True
        }
    }
}

app = App.from_config(config=config)
# Add your data source here
app.add("https://docs.embedchain.ai/sitemap.xml", data_type="sitemap")
app.query("Write a poem about Embedchain")

# In the realm of data, vast and wide,
# Embedchain stands with knowledge as its guide.
# A platform open, for all to try,
# Building bots that can truly fly.

# With REST API, data in reach,
# Deployment a breeze, as easy as a speech.
# Updating data sources, anytime, anyday,
# Embedchain's power, never sway.

# A knowledge base, an assistant so grand,
# Connecting to platforms, near and far.
# Discord, WhatsApp, Slack, and more,
# Embedchain's potential, never a bore.
```
</CodeGroup>

## NVIDIA AI

[NVIDIA AI Foundation Endpoints](https://www.nvidia.com/en-us/ai-data-science/foundation-models/) let you quickly use NVIDIA's AI models, such as Mixtral 8x7B, Llama 2 etc, through our API. These models are available in the [NVIDIA NGC catalog](https://catalog.ngc.nvidia.com/ai-foundation-models), fully optimized and ready to use on NVIDIA's AI platform. They are designed for high speed and easy customization, ensuring smooth performance on any accelerated setup.


### Usage

In order to use LLMs from NVIDIA AI, create an account on [NVIDIA NGC Service](https://catalog.ngc.nvidia.com/).

Generate an API key from their dashboard. Set the API key as `NVIDIA_API_KEY` environment variable. Note that the `NVIDIA_API_KEY` will start with `nvapi-`.

Below is an example of how to use LLM model and embedding model from NVIDIA AI:

<CodeGroup>

```python main.py
import os
from embedchain import App

os.environ['NVIDIA_API_KEY'] = 'nvapi-xxxx'

config = {
    "app": {
        "config": {
            "id": "my-app",
        },
    },
    "llm": {
        "provider": "nvidia",
        "config": {
            "model": "nemotron_steerlm_8b",
        },
    },
    "embedder": {
        "provider": "nvidia",
        "config": {
            "model": "nvolveqa_40k",
            "vector_dimension": 1024,
        },
    },
}

app = App.from_config(config=config)

app.add("https://www.forbes.com/profile/elon-musk")
answer = app.query("What is the net worth of Elon Musk today?")
# Answer: The net worth of Elon Musk is subject to fluctuations based on the market value of his holdings in various companies.
# As of March 1, 2024, his net worth is estimated to be approximately $210 billion. However, this figure can change rapidly due to stock market fluctuations and other factors.
# Additionally, his net worth may include other assets such as real estate and art, which are not reflected in his stock portfolio.
```
</CodeGroup>

## Zhipu AI


### Usage

In order to use LLMs from Zhipu AI, create an account on [Zhipu AI open platform](https://open.bigmodel.cn/).

Generate an API key from their dashboard. Set the API key as `ZHIPU_API_KEY` environment variable.

Below is an example of how to use LLM model and embedding model from Zhipu AI:

<CodeGroup>

```python main.py
from embedchain import App
import os
os.environ["ZHIPU_API_KEY"]="your-zhipu-api-key"
config = {
  'llm': {
    'provider': 'zhipuai',
    'config': {
      'model': 'glm-4',
      'temperature':0.1,
      'top_p':0.1,
      "stream":False,
    }
  },
  'embedder': {
    'provider': 'huggingface',
    'config': {
      'model': 'sentence-transformers/all-mpnet-base-v2'
    }
  }
}
app = App.from_config(config=config)
app.add("https://baike.baidu.com/item/%E5%9F%83%E9%9A%86%C2%B7%E9%A9%AC%E6%96%AF%E5%85%8B/3776526?fr=ge_ala")
"""测试用例：
>>> app.query("马斯克在2001年初做了什么")
在2001年初，马斯克还在贝宝期间，策划了一个名为“火星绿洲”的项目，计划将一个小型实验温室降落在火星上，以尝试让地球的农作物在火星土壤中生长。然而，当他发现购买俄罗斯宇航公司运载火箭的成本远高于自行研发火箭的成本，且发射成本比项目的研发和工程成本还要高时，他决定暂缓这个项目，并成立SpaceX公司，以研究如何降低发射成本。

>>> app.query("SpaceX研制龙飞船2号的时间是什么时候")
SpaceX从2014年开始研制飞船2号。

"""


"""大模型内部详情。
# 第一个示例的user prompt
You are a Q&A expert system. Your responses must always be rooted in the context provided for each query. Here are some guidelines to follow:

1. Refrain from explicitly mentioning the context provided in your response.
2. The context should silently guide your answers without being directly acknowledged.
3. Do not use phrases such as 'According to the context provided', 'Based on the context, ...' etc.

Context information:
----------------------
[11]1984年，马斯克成功设计出一个名叫“炸弹”（Blastar）的太空游戏软件，之后以500美元的价格出售给《PC and Office Technology》杂志，赚到人生的第一桶金。 [12] [189]少年时代少年时代少年时代（左）少年马斯克（右一）与母亲（右二）婴儿时期的马斯克与父亲教育经历高中时期，马斯克先在南非约翰内斯堡北郊的布兰斯顿高中（Bryanston High School）就读， [190]因为受到严重欺负，就读两年后，父亲帮他转学到一所私⽴学校—比勒陀利亚男⼦中学，这所学校以英国教学模式为基础，校规严格，在那⾥，他的学习成绩都很好，只有两⻔课除外—南非语（毕业那年的百分制考试，他只得了61分）和宗教教育（⽼师说他对于教诲“闭目塞听”）。 [190] [191]大学时期的马斯克1988年，马斯克从比勒陀利亚男子高中毕业后，由于没有父母资助加上义务兵役的缘故，他选择离开家庭。 [3] [13]。1989年，获得加拿大国籍后，只身前往加拿大，寄居于母亲亲戚家中，并于次年申请进入位于安大略省的女王大学 [4]。1992年，马斯克依靠奖学金转入美国宾夕法尼亚大学沃顿商学院攻读经济学，大学期间，他开始深入关注互联网、清洁能源、太空这三个影响人类未来发展的领域 [3]。在取得经济学学士学位后，又留校一年拿到物理学学士学位。 [4]1995年，马斯克进入斯坦福大学，攻读材料科学和应用物理 [14]博士课程，但在入学后的第2天，就决定离开学校开始创业。 [3]工作经历播报编辑创办Zip21995年，马斯克辍学后，与弟弟卡姆巴·马斯克（Kimbal Musk）拿到硅谷一个小集团的随机天使投资 [15]，创办了Zip2公司，这是一家为新闻机构开发在线内容出版软件的公司，当时《纽约时报》和《芝加哥邮报》都成为了马斯克兄弟的客户。四年后，美国电脑制造商康柏公司以3.07亿美元现金和3400万美元股票期权收购了Zip2公司，28岁的埃隆·马斯克在这笔收购中获利2200万美元。 [4]创办X.com1999年3月，埃隆·马斯克投资1000万美元，与两位来自硅谷的合伙人创办了一家在线金融服务和电子邮件支付业务公司“X.com”。 [6]2000年，为解决在网上快捷转账业务上的竞争，马斯克将X.com公司与彼得·蒂尔和麦克斯·拉夫琴创办的Confinity公司合并。2001年，新公司更名为贝宝（PayPal）。2002年初，PayPal上市 [4]。2002年10月，PayPal被当时全球最大的网商公司易贝（eBay）以15亿美元全资收购，埃隆·马斯克作为当时拥有贝宝11.7%股权的最大股东，拿到了1.65亿美元。 [4]在这笔交易完成后，贝宝的许多核心成员离开了公司，马斯克在彼得·蒂尔离职后接任CEO一职，但之后其在公司内部的斗争中失败而被逐出公司。 [16]创办PayPal时期的埃隆·马斯克创办SpaceX火星绿洲马斯克在大学曾修过物理学，但他并不是火箭专家。2001年初，马斯克还在贝宝期间，就策划了一个叫做“火星绿洲”的项目，计划把一个小型实验温室降落在火星上，让来自地球的农作物在火星土壤里试着生长。不过当他发现去俄罗斯宇航公司购买运载火箭的成本大大高出自行研发火箭的成本，且发射成本比这个项目的研发和工程成本都要高得多的时候，暂缓了这个项目，决定先成立一个公司来研究怎样降低发射成本，这就是SpaceX公司。 [4] [26]Space X2002年6月，马斯克投资1亿美元成立SpaceX，任首席执行官兼首席技术官，开始研究如何降低火箭发射成本，并计划在未来实现火星移民，打造人类真正的太空文明。 [17]猎鹰1号2006年3月24日，SpaceX的猎鹰1号火箭第一次发射失败，原因是一枚铝制螺栓在发射前被海水腐蚀到出现裂纹，导致第一级火箭发射后管道漏油着火，摧毁了马斯克想迅速成功发射的火箭梦。2007年3月20日，SpaceX迎来了第二次发射失败，在此次发射中，第一级火箭完成使命，一、二级火箭成功分离、二级发动机顺利点火、卫星整流罩分离，最终因为小的偏差导致第二级火箭并没有能够成功入轨。2008年8月2日，SpaceX第三次发射失败，原因是没有为一二级火箭分离留下两三秒的足够时间差。2008年9月28日，SpaceX第四次发射猎鹰1号，此次发射在马绍尔群岛的欧姆雷克岛（Omelek Island）上获得成功，这是私人投资公司首次成功进行轨道发射，代表着政府项目主导的航天行业发生了重大转变。在首次成功发射后，SpaceX从NASA和一些私人投资者那里获得了更多资金，开启了高速发展之路。2009年，猎鹰1号火箭在第五次发射后退役。 | [102] [122] [131] [153-154]福布斯财富榜单时间财富排名2024年1950亿美元2024福布斯全球亿万富豪榜第2位 [165]2023年1800亿美元2023福布斯全球亿万富豪榜第2位 [166]2022年2190亿美元2022福布斯全球亿万富豪榜第1位 [68]2021年1510亿美元2021福布斯全球亿万富豪榜第2位 [46]2020年246亿美元2020福布斯全球亿万富豪榜第31位 [34]2019年223亿美元2019福布斯全球亿万富豪榜第40位 [157]2018年199亿美元2018福布斯全球亿万富豪榜第54位 [167]2017年139亿美元2017年福布斯全球富豪排行榜第80位 [168]胡润财富榜单时间财富排名2024年16700亿元人民币2024胡润全球富豪榜第1位 [141]2023年10500亿元人民币2023胡润全球富豪榜第2位 [105]2022年12900亿元人民币2022年胡润全球富豪榜第1位 [66]2021年12800亿元人民币2021胡润全球富豪榜第1位 [45]2020年3220亿元人民币2020胡润全球富豪榜第20位 [158]2019年1900亿元人民币2019胡润全球富豪榜第30位 [159]2018年1500亿元人民币2018胡润全球富豪榜第40位 [160]2017年1000亿元人民币2017胡润全球富豪榜第75位 [161]2016年580亿元人民币2016胡润全球富豪榜第134位 [163]2015年390亿元人民币2015胡润全球富豪榜第190位 [162]2014年395亿元人民币2014胡润全球富豪榜第212位 [164]人物评价播报编辑埃隆有一种非常强烈的专注模式（intense mode），我也有这样的时候。所有成功者都有这样一面。他们必须如此，否则就不会成功。这种模式会让他只关注如何取得成功，不及其余。这也可说是一种“聚焦”，心神极度集中，就像放大镜将光线汇聚于纸上一点。（马斯克的父亲埃罗尔·马斯克 评） [183]马斯克用专注替代了亲密，而专注的当然是事业。（马斯克的第一任妻子贾斯汀·威尔逊 评） [186]他内心依然是个男孩，一个站在父亲面前的孩子。（马斯克的第二任妻子妲露拉·莱莉 评） [183]我只是觉得他不懂得如何享受成功，嗅闻花香。（马斯克前女友格莱姆斯 评） [183]马斯克是一个目标坚定、以工程师视角看世界的疯狂创业者，他在大学时期就思考人类未来的大问题，并看好互联网、可持续能源和空间探索领域，后来也依次取得了重大成就。他对于反对者会坚定反击，表达自己的真实想法。（马斯克在宾夕法尼亚大学的室友Adeo Ressi 评） [7]埃隆·马斯克是全世界的“伟大天才”之一，他就像是爱迪生，他也“是我们伟大的天才之一，而我们必须保护自己的天才”。（时任美国总统唐纳德·特朗普 评） [118] [128]埃隆最了不起的一点就是能把自己的愿景当作上苍的旨意。（Paypal联合创始人马斯克·列夫琴 评） [183]马斯克是为了冒险而冒险，他似乎享受其中，有时甚至是上瘾。他对风险的理解是我们大多数人都不具备的。（Paypal共同创始人彼得·蒂尔 评） [183] [186]拥抱风险需要赌性，而马斯克属于那种习惯于堵上身家性命，而且不愿意下场休息的那种，也只有这么做，才能获得一系列的成功。（领英创始人霍夫曼 评） [186]马斯克是一位非常了不起的企业家，而特斯拉是一家在汽车工业史上划时代的伟大的公司。我和马斯克一样，都帮助了科技的普及，推动了这个社会的进步。（小米创始人雷军 评） [184]有一种人，智商达到190，但是他们以为自己的智商有250。这种人，我最怕了，马斯克有点类似这种人。（投资家查理·芒格 评） [186]马斯克专横、情绪化、爱面子、缺乏安全感、自大、偏执，是一个很难相处的人。似乎离开特斯拉的人，没有想再回去的。马斯克很难界定是坏人，还是英雄。他身上有很多灰色地带，很多时候取决于他当天的心情。他拥有一种罕见的能力，就是嫁接工程师和销售人员的能力。既能跟工程师沟通技术问题，也能向外界展示和营销电动汽车。能推动尖端科技发展，也能把尖端科技分享给普罗大众。（美国资深财经评论员、《华尔街日报》科技与汽车专栏记者提姆·希金斯 评） [185]人物事件播报编辑阻碍科技创新奖2016年，马斯克因为对人工智能技术的态度而获得了“阻碍技术创新”奖，即“卢德奖”。他和史蒂芬·霍金等人因为对人工智能保持高度警惕，并表达了对此技术的担忧，而被认为在一定程度上阻碍技术创新。他们担心人工智能的发展可能超出人类控制，并呼吁对自动化武器等技术进行限制。 | 京ICP证030173号 京公网安备11000002000001号
----------------------

Query: 马斯克在2001年初做了什么
Answer:
# 第一个示例的结果
在2001年初，马斯克还在贝宝期间，策划了一个名为“火星绿洲”的项目，计划将一个小型实验温室降落在火星上，以尝试让地球的农作物在火星土壤中生长。然而，当他发现购买俄罗斯宇航公司运载火箭的成本远高于自行研发火箭的成本，且发射成本比项目的研发和工程成本还要高时，他决定暂缓这个项目，并成立SpaceX公司，以研究如何降低发射成本。
# 第二个示例的user prompt
You are a Q&A expert system. Your responses must always be rooted in the context provided for each query. Here are some guidelines to follow:

1. Refrain from explicitly mentioning the context provided in your response.
2. The context should silently guide your answers without being directly acknowledged.
3. Do not use phrases such as 'According to the context provided', 'Based on the context, ...' etc.

Context information:
----------------------
[192-195]猎鹰9号埃隆·马斯克与猎鹰运载火箭猎鹰1号发射成功之后，SpaceX就开始升级，将原本仅有1台梅林（Merlin）发动机的猎鹰1号升级为由9台梅林发动机组成的猎鹰9号。2010年6月4日猎鹰9号完成首次发射，2011年SpaceX又拿到追加至3.96亿美元的投资，用于研发猎鹰9号和龙飞船。一般情况下，火箭发射升空后，第一级推进器就会被废弃，马斯克认为这是火箭发射成本高的主要原因，若能将其回收将有效降低成本。从2012年9月到2013年10月，SpaceX陆续做了8次火箭回收试验，最终在2015年12月21日，猎鹰9号安全在陆地软着陆，这也是人类发射火箭历史上的第一次助推器回收。2016年4月，猎鹰9号搭载龙飞船顺利升空，一级火箭助推器分离之后，稳稳降落在海上回收平台，实现了首次海上回收。2016年，SpaceX经历了被马斯克称为“最困难最复杂”的一次失败。当年9月1日，猎鹰9号在发射前的测试中爆炸了，事故造成Facebook价值2亿美元的Amos-6卫星完全摧毁，SpaceX也由此陷入了4个月的停滞，直至次年1月才重新开始发射试验。在无数次的爆炸和失败中，猎鹰9号逐渐成为SpaceX最为成熟的火箭，担负了SpaceX的主要发射任务。 [195]重型猎鹰2011年，SpaceX旗下的大运力火箭“重型猎鹰”（Falcon Heavy）首次对外亮相，“重型猎鹰”高69.2米，近地轨道载荷达63.8吨，采用芯级并联的结构设计，由三枚经改装的“猎鹰9号”火箭组装而成。2015年和2016年的一些火箭故障迫使公司推迟发射。在过去几年中，SpaceX一直在更新火箭的目标发射日期；到了2017年年底，又因为美国政府停摆，马斯克暂停了发射。直至2018年伊始，SpaceX才首次公布一个具体发射日期。 [196]2018年2月7日，“重型猎鹰”运载火箭在美国肯尼迪航天中心首次成功发射，并成功完成两枚一级助推火箭的完整回收。 [8]龙飞船马斯克在SPACEX前留影2012年10月7日，SpaceX从卡纳维拉尔角空军基地发射了猎鹰9号火箭，将龙飞船送入轨道，这是SpaceX公司货运飞船首次正式承担向国际太空站运货的任务 [197]，也是第一艘向国际空间站运送货物的商业飞船，并促使NASA和SpaceX又签订了几份合同。龙飞船1号在2020年退役，之前向国际空间站执行了23次货运任务。SpaceX公司从2014年开始研发龙飞船2号，这是一种能够搭载宇航员和货物的版本。2020年5月30日，SpaceX使用猎鹰9号发射了新的载人龙飞船，上面搭载两名宇航员道格·赫尔利（Doug Hurley）和鲍勃·本肯（Bob Behnken）。这两人是第一批乘坐SpaceX火箭升空的人类。 [193]2021年4月23日，载有4名宇航员的SpaceX龙飞船在肯尼迪航天中心发射升空，飞船飞往国际空间站，此次载人飞行任务也是SpaceX与美国国家航空航天局进行的第二次商业载人航天发射任务。 [198]2021年9月15日，亿万富翁贾里德·伊萨克曼（Jared Issacman）和其他三名乘客包下SpaceX的一次私人航天旅行，在太空中度过了三天，SpaceX使用猎鹰9号将他们送入太空，乘坐的是“坚韧号”载人龙飞船，这次太空飞行是第一次没有任何专业宇航员的情况下到达轨道的载人飞行任务。 [193]星际飞船Starship2019年底， SpaceX开始研发新项目，着手建造下一代星际飞船Starship。Starship飞船长50米，直径9米，由合金钢制成，发射载荷为150吨，可重复使用，用于火星飞行，可容纳100人。 [195]2021年5月，SpaceX首次在没有发生爆炸的情况下完成了Starship的亚轨道试飞，此前四次，Starship原型都发生了爆炸。马斯克的长期目标是在火星上建造自给自足的定居点，为了将足够的人员和物资送往这颗红色星球，SpaceX需要一枚强大且完全可重复使用的火箭，于是，Starship和超重型猎鹰火箭的组合体登场。 [193]星链网络2015年1月，马斯克宣布SpaceX的卫星互联网服务的计划。SpaceX计划将约1.2万颗通信卫星发射到轨道，并从2020年开始工作，这一项目被命名为“星链”（Starlink），意图在全球范围内提供低成本的互联网连接服务。 [200]2018年3月，猎鹰9号火箭在一次常规发射任务中搭载了2颗小卫星—“丁丁-a”和“丁丁-b”，它们是SpaceX星链计划的试验星，主要开展对地通信测试 [201]。2019年，SpaceX开始发射星链卫星 | [193]。2020年5月，SpaceX向美国联邦通信委员会提交第二代“星链”卫星部署计划申请，希望额外部署3万颗卫星，将“星链”卫星数量扩充至4.2万颗。2022年12月，美国联邦通信委员会批准“星链”项目部署至多7500颗第二代卫星，其余暂缓决定，以回应“有关轨道碎片和航天安全的担忧”。 [202]截至2024年4月，该公司已经向近地轨道发射了6145颗星链卫星。 [199]投资特斯拉2004年，马斯克向马丁·艾伯哈德创立的电动汽车制造商特斯拉汽车投资630万美元，条件是他出任该公司的董事长，并拥有所有事务的最终决定权。 [4] [6]2008年，特斯拉推出第一款电动跑车Roadster，但因为金融危机蔓延，特斯拉高昂的研发成本，一度让公司陷入困境，面临破产风险 [203]。为了解决资金短缺，马斯克把他个人最后的4000万美元投了进去，并担任特斯拉CEO，带领特斯拉挺过最艰难的时刻。2010年初，时任美国总统奥巴马参观特斯拉工厂，随后特斯拉成功拿下4.65亿美元的政府低息贷款，之后Model S开始接受预定 [204]。2010年6月，马斯克成功带领特斯拉在纳斯达克上市，筹集了约1.84亿美元资金。尽管上市前夕有报道称他濒临个人破产，但特斯拉成功上市为他带来了转机。上市后，马斯克的个人财富大幅增长，特斯拉也成为自1956年福特汽车IPO以来，第一家上市的美国汽车制造商，并且是当时唯一一家在美国上市的纯电动汽车独立制造商。 [18]马斯克与特斯拉2012年，特斯拉正式推出了Model S。随着这一车型的成功，特斯拉股价不断上涨，在2013年冲上了每股158美元的历史高点，同年7月，特斯拉被纳入纳斯达克100指数，成为唯一进入这一指数的美国汽车股，市值很快超过100亿美元。 [205]2014年，特斯拉宣布共享专利技术，推动电动车发展，其开放专利的行为被视为意图成为行业标准 [212-213]。随着特斯拉全球工厂不断建立，2019年，特斯拉上海超级工厂以“特斯拉”速度，实现开工、投产、交付三步走，成为中国第一家外商独资设立的汽车公司，也是特斯拉首个海外工厂。 [206]2020年6月10号，特斯拉股价首破1000美元，成为全球市值最高车企 [207]。特斯拉公司逐渐形成了Model S，Model X，Model Y，Model 3等新能源车阵营，此外还研发销售太阳能电池板，太阳能屋顶等清洁能源产品。 [209]创办SolarCity马斯克旗下三大重要公司2006年，马斯克个人出资1000万美元，创办了太阳城公司（SolarCity），由他的表弟Lyndon Rive担任CEO和联合创始人。其目标是让千家万户使用太阳能发电系统，“加速可持续能源时代的到来”。2016年1月，SolarCity已发展为美国太阳能发电系统供应商龙头，在加州、亚利桑那州和俄勒冈州的500个社区提供服务。 [4]2016年8月，特斯拉公司宣布，与太阳城公司达成一项价值26亿美元的并购协议，这项交易被媒体形容为“左手牵右手”，因为太阳城公司董事会主席及控股股东正是特斯拉首席执行官埃隆·马斯克。马斯克表示，这项交易将加速特斯拉从电动汽车制造商向一体化可再生能源公司的转变。特斯拉与太阳城的整合，将改变能源生产、存储和消费模式，其最终目标是打造全球唯一的垂直一体化能源公司，产品覆盖太阳能面板、家用蓄电池和电动车等。 [208]创办轨交公司2016年12月，马斯克成立了一家解决地面拥堵问题的轨道交通公司The Boring Company [19]，根据其设想，他将在地面上安装汽车暂停的“托盘”，汽车停好后，托盘会下降到地底，将车子在地底隧道间快速运输，最快时速甚至到达200千米。 [19]Boring公司由马斯克在SpaceX的长期副手斯蒂夫·戴维斯（Steve Davis）掌舵。2017年7月，马斯克在推特上表示，他得到了“政府口头批准”，允许Boring公司开始建造超级高铁（Hyperloop）。超级高铁是马斯克基于上世纪70年代的一项提议重新提出的概念，要求载具以每小时约700英里的速度通过真空管运送乘客。2017年秋天，马里兰州加快Boring项目的审批程序，于2017年10月发放了有条件的许可证，几个月后又发放了环境许可证，但该项目直至2023年12月，仍只有可供演示的2.4英里（约合3.8公里）长隧道。 [216-217]2023年12月，马斯克超级高铁梦想破碎，Hyperloop One（前身为Virgin Hyperloop）被曝正在出售其资产、解雇剩余员工，并准备在2023年底前关闭。 [243]投资人工智能DeepMind2010年9月，DeepMind成立，马斯克是其首批投资人。2014年，DeepMind被谷歌以6亿美元的价格收购。 | [93]，马斯克入主推特后给这家公司带来巨变，一系列的降本增效措施让推特员工骤减一半以上，也让推特陷入了一定程度上的困境之中。 [97]2022年12月20日晚，马斯克在推特发文称，自己将辞去推特CEO一职，但前提是找到“傻到可以接任这项工作”的人。 [95-96]2023年10月31日，推特公司董事会解散，马斯克成为推特的唯一董事。 [124]涉足影视除了众多商业公司外，马斯克还在影视方面展现出多元化的参与和影响力，他曾为三部电影当过制作人，与好莱坞的关系密切，影星乔治·克鲁尼、导演卡梅隆都是他的座上宾，在互联网电影资料库里搜索他的名字，有长长一串结果。此外，他还在7部电影和电视剧中客串了他自己。当《钢铁侠》的导演为了丰富角色找到埃隆·马斯克时，他对当制作人表达了强烈兴趣，电影中一部分镜头是在SpaceX总部空旷的厂区拍摄，最后的字幕表里，埃隆·马斯克的名字列在“特别感谢”一栏。埃隆·马斯克在影视作品中客串自己社会活动播报编辑公益慈善马斯克基金会马斯克基金会（Musk Foundation）是马斯克的主要慈善工具，成立于2001年。该基金会简单又不透明，一度有人质疑马斯克是否在做慈善，基金会网站以纯文本格式放在Yahoo页面上，没有链接，没有工作人员信息，也没有联系表格，只有短短几行字，列出了基金会的捐助方向，包括：可再生能源、太空探索、儿科医学、STEM理工科教育以及造福人类的人工智能。由马斯克的弟弟金巴尔·马斯克（Kimbal Musk）担任基金会司库。根据美国国税局（IRS）2001年至2017年的报告统计显示，马斯克基金会在15年间累计向160个慈善机构捐出了5400万美元，其中有三分之一是直接捐款，捐款额度为几千美元不等。这些慈善机构包括环保、教育、医疗以及航天等等。马斯克基金会常使用DAF（Donor Advised Fund，捐赠人建议基金，一种可以让捐赠者管理慈善资金如何使用的基金管理公司）进行慈善捐赠。 [169-170]捐献誓言2012年，马斯克响应盖茨和巴菲特的号召，签署了“捐赠承诺”（The Giving Pledge），承诺在有生之年将自己至少一半的资产捐给慈善机构。签署人可以自由选择自己的慈善捐赠对象，既可以是医学研究项目，也可以是非洲扶贫项目，也可以是社会公益活动，或者是教育平等项目。但他是少数几个不在该项目网站上公开其承诺书的签署者之一。 [169-170]社交媒体发奖马斯克定期在推特上，向那些呼吁他参与慈善的特斯拉粉丝透露他与慈善相关的想法。有时候，还会承诺将自己基金会的钱奖励给那些在推特上向他发问的粉丝。 [170]支持OpenAI2015年，非常关注人工智能技术前景的马斯克和几位富豪一道承诺捐赠10亿美元创建OpenAI人工智能研究公司，以便以“最有可能造福人类的方式”研究安全的人工智能技术。其中，马斯克捐赠了1000万美元。 [169-170]大额捐赠2021年11月，马斯克捐出504万股特斯拉股票用于慈善。按照当时股价，这部分股票价值超过57.4亿美元。但马斯克没有透露这笔捐款的去向，也没有回应媒体的质询。 [169]教育捐赠2020年，马斯克向洛杉矶的几所私立学校进行了捐款，包括十字路艺术与科学学校（Crossroads School for Arts and Sciences）和迎风学校（The Windward School）。他的基金会为圣心大学（Sacred Heart University）的一项新冠病毒抗体研究拨款5万美元。另外，马斯克向Ad Astra School捐赠6万美元，这是他于2014年在SpaceX洛杉矶校区与他人合作创办的一所实验性私立学校，其子女和一些SpaceX员工的子女都在这所学校里就读。 [171]2022年10月，马斯克旗下基金会提交了在得克萨斯州奥斯汀创办一所大学的申请，并于2023年3月获得批准。根据申请文件，该基金会会利用马斯克捐赠的约1亿美元资金作为种子资金，先创办一所以科学、技术、工程和数学为重点的小学和中学。学校一旦开始运营，基金会“计划最终扩大业务，创建一所致力于最高水平教育的大学”。该大学将聘请“经验丰富的教师”，并以传统课程为特色，“以及包括模拟、案例研究、制造/设计项目和实验室在内的实践学习体验”。 [172]其他捐赠马斯克表示，他是美国公民自由联盟的“顶级捐赠者之一”，但他并没有透露具体的捐赠数额。马斯克也从未登上《福布斯》的顶级慈善家排行榜。下表列出他一些主要的捐赠项目。 [170-171]马斯克的捐赠项目（部分）捐赠项目&机构捐赠金额生命未来研究所（Future of Life Institute），主要研究人工智能安全至少1000万美元一个专注于促进全球扫盲的奖项1000万美元环境保护组织塞拉俱乐部（Sierra
----------------------

Query: SpaceX研制龙飞船2号的时间是什么时候
Answer:
# 第二个示例的结果
SpaceX从2014年开始研发龙飞船2号。
"""

config["llm"]["config"]["stream"]=True
app = App.from_config(config=config)
app.add("https://baike.baidu.com/item/%E5%9F%83%E9%9A%86%C2%B7%E9%A9%AC%E6%96%AF%E5%85%8B/3776526?fr=ge_ala")
"""
>>> result = app.query("马斯克在2001年初做了什么")
>>> for item in result:
>>>     print(item)
"""
```
</CodeGroup>
<br/ >

<Snippet file="missing-llm-tip.mdx" />
